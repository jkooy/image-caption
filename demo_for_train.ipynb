{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: train.py [-h] [--model_path MODEL_PATH] [--crop_size CROP_SIZE]\r\n",
      "                [--vocab_path VOCAB_PATH] [--train_image_dir TRAIN_IMAGE_DIR]\r\n",
      "                [--val_image_dir VAL_IMAGE_DIR]\r\n",
      "                [--train_caption_path TRAIN_CAPTION_PATH]\r\n",
      "                [--val_caption_path VAL_CAPTION_PATH] [--log_step LOG_STEP]\r\n",
      "                [--save_step SAVE_STEP] [--embed_size EMBED_SIZE]\r\n",
      "                [--hidden_size HIDDEN_SIZE] [--num_layers NUM_LAYERS]\r\n",
      "                [--num_epochs NUM_EPOCHS] [--batch_size BATCH_SIZE]\r\n",
      "                [--num_workers NUM_WORKERS] [--learning_rate LEARNING_RATE]\r\n",
      "                [--model_file MODEL_FILE]\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  --model_path MODEL_PATH\r\n",
      "                        path for saving trained models\r\n",
      "  --crop_size CROP_SIZE\r\n",
      "                        size for randomly cropping images\r\n",
      "  --vocab_path VOCAB_PATH\r\n",
      "                        path for vocabulary wrapper\r\n",
      "  --train_image_dir TRAIN_IMAGE_DIR\r\n",
      "                        directory for resized images\r\n",
      "  --val_image_dir VAL_IMAGE_DIR\r\n",
      "                        directory for resized images\r\n",
      "  --train_caption_path TRAIN_CAPTION_PATH\r\n",
      "                        path for train annotation json file\r\n",
      "  --val_caption_path VAL_CAPTION_PATH\r\n",
      "                        path for train annotation json file\r\n",
      "  --log_step LOG_STEP   step size for prining log info\r\n",
      "  --save_step SAVE_STEP\r\n",
      "                        step size for saving trained models\r\n",
      "  --embed_size EMBED_SIZE\r\n",
      "                        dimension of word embedding vectors\r\n",
      "  --hidden_size HIDDEN_SIZE\r\n",
      "                        dimension of lstm hidden states\r\n",
      "  --num_layers NUM_LAYERS\r\n",
      "                        number of layers in lstm\r\n",
      "  --num_epochs NUM_EPOCHS\r\n",
      "  --batch_size BATCH_SIZE\r\n",
      "  --num_workers NUM_WORKERS\r\n",
      "  --learning_rate LEARNING_RATE\r\n",
      "  --model_file MODEL_FILE\r\n",
      "                        path for trained encoder\r\n"
     ]
    }
   ],
   "source": [
    "! python train.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=128, crop_size=240, embed_size=256, hidden_size=512, learning_rate=0.001, log_step=200, model_file=None, model_path='./demo_train_SavedModel', num_epochs=5, num_layers=1, num_workers=4, save_step=200, train_caption_path='/datasets/ee285f-public/COCO-Annotations/annotations_trainval2014/captions_train2014.json', train_image_dir='/datasets/COCO-2015/train2014', val_caption_path='/datasets/ee285f-public/COCO-Annotations/annotations_trainval2014/captions_val2014.json', val_image_dir='/datasets/COCO-2015/val2014', vocab_path='./vocab.pkl')\n",
      "loading annotations into memory...\n",
      "Done (t=1.07s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.72s)\n",
      "creating index...\n",
      "index created!\n",
      "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /tmp/xdg-cache/torch/checkpoints/resnet152-b121ed2d.pth\n",
      "100%|████████████████████████████████████████| 230M/230M [00:03<00:00, 79.7MB/s]\n",
      "Training loss: 9.199362754821777\n",
      "Val loss: 9.10242748260498\n",
      "Val loss: 9.106578826904297\n",
      "Val loss: 9.103963851928711\n",
      "Val loss: 9.107028007507324\n",
      "Val loss: 9.10451602935791\n",
      "Val loss: 9.100770950317383\n",
      "Val loss: 9.103009223937988\n",
      "Val loss: 9.103504180908203\n",
      "Val loss: 9.103705406188965\n",
      "Val loss: 9.104947090148926\n",
      "Val loss: 9.107014656066895\n",
      "Val loss: 9.103424072265625\n",
      "Val loss: 9.104438781738281\n",
      "Val loss: 9.103687286376953\n",
      "Val loss: 9.105744361877441\n",
      "Val loss: 9.104121208190918\n",
      "Val loss: 9.103628158569336\n",
      "Val loss: 9.105646133422852\n",
      "Val loss: 9.102359771728516\n",
      "Val loss: 9.107163429260254\n",
      "Val loss: 9.100019454956055\n",
      "Val loss: 9.10599422454834\n",
      "Val loss: 9.103129386901855\n",
      "Val loss: 9.10819149017334\n",
      "Val loss: 9.109387397766113\n",
      "Val loss: 9.100146293640137\n",
      "Val loss: 9.102911949157715\n",
      "Val loss: 9.101377487182617\n",
      "Val loss: 9.102426528930664\n",
      "Val loss: 9.098273277282715\n",
      "Val loss: 9.101799964904785\n",
      "Val loss: 9.098165512084961\n",
      "Val loss: 9.104049682617188\n",
      "Val loss: 9.107409477233887\n",
      "Val loss: 9.110694885253906\n",
      "Val loss: 9.101862907409668\n",
      "Val loss: 9.10395622253418\n",
      "Val loss: 9.106151580810547\n",
      "Val loss: 9.102073669433594\n",
      "Val loss: 9.108064651489258\n",
      "Training loss: 9.100935935974121\n",
      "Training loss: 8.981097221374512\n",
      "Training loss: 8.850029945373535\n",
      "Training loss: 8.615096092224121\n",
      "Training loss: 8.200864791870117\n",
      "Training loss: 7.553869247436523\n",
      "Training loss: 6.892033100128174\n",
      "Training loss: 6.359524726867676\n",
      "Training loss: 6.021094799041748\n",
      "Training loss: 5.767900466918945\n",
      "Training loss: 5.749682426452637\n",
      "Training loss: 5.609550476074219\n",
      "Training loss: 5.641334533691406\n",
      "Training loss: 5.571266174316406\n",
      "Training loss: 5.614274978637695\n",
      "Training loss: 5.360207557678223\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 170, in <module>\n",
      "    main(args)\n",
      "  File \"train.py\", line 65, in main\n",
      "    for i, (images, captions, lengths) in enumerate(train_data_loader):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 804, in __next__\n",
      "    idx, data = self._get_data()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 771, in _get_data\n",
      "    success, data = self._try_get_data()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 724, in _try_get_data\n",
      "    data = self._data_queue.get(timeout=timeout)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 920, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/conda/lib/python3.7/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "! python train.py\n",
    "\n",
    "# we have included all the default parameters and addresses of files needed during training.\n",
    "# when run this python file, the training for the model: resnet152+lstm+hidden_size512+lr_1e-3 will start\n",
    "# in our project, train this model will take about 4 hours to finish run all five epochs\n",
    "\n",
    "'''\n",
    "Namespace(batch_size=128, crop_size=240, embed_size=256, hidden_size=512, \n",
    "learning_rate=0.001, log_step=200, model_file=None, model_path='./demo_train_SavedModel', \n",
    "num_epochs=5, num_layers=1, num_workers=4, save_step=200, \n",
    "train_caption_path='/datasets/ee285f-public/COCO-Annotations/annotations_trainval2014/captions_train2014.json', \n",
    "train_image_dir='/datasets/COCO-2015/train2014', \n",
    "val_caption_path='/datasets/ee285f-public/COCO-Annotations/annotations_trainval2014/captions_val2014.json', \n",
    "val_image_dir='/datasets/COCO-2015/val2014', vocab_path='./vocab.pkl')\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
